{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TH NÃ¼rnberg - Neural Network Translator - Christoph Brandl, Philipp Grandeit\n",
    "\n",
    "# Build sample Neural Networks for the Neural-Network-Translator with Keras\n",
    "\n",
    "This jupyter notebooks provides everything you need to build sample neural networks with TensorFlow (Keras) which you can later translate with the Neural-Network-Translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an out directory to save the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('out'):\n",
    "    os.makedirs('out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a neural network only consisting of one single average pooling 1D layer. The created model can then be used to validate the implementation of the average pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will prepare all the necessary data to build and \"train\" this simple neural network. Since the pooling layer is a linear process, no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,2,3,4,5,6]])\n",
    "data_y = np.array([[1,2,3]])\n",
    "\n",
    "# data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,6,1])\n",
    "data_y = tf.reshape(data_y,[-1,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.AveragePooling1D(pool_size=3, strides=1, padding='same', input_shape = (6,1)))\n",
    "# model.add(keras.layers.AveragePooling1D(pool_size=3, strides=4, padding='same', input_shape = (5,1)))\n",
    "# model.add(keras.layers.AveragePooling1D(pool_size=3, strides=2, padding='same', input_shape = (4,1)))\n",
    "# model.add(keras.layers.AveragePooling1D(pool_size=3, strides=2, padding='same', input_shape = (6,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "average_pooling1d_4 (Average (None, 6, 1)              0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.5],\n",
       "        [2. ],\n",
       "        [3. ],\n",
       "        [4. ],\n",
       "        [5. ],\n",
       "        [5.5]]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/avg_pool_1d_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'class_name': 'InputLayer',\n",
       "  'config': {'batch_input_shape': [None, 6, 1],\n",
       "   'dtype': 'float32',\n",
       "   'sparse': False,\n",
       "   'ragged': False,\n",
       "   'name': 'average_pooling1d_input'}},\n",
       " {'class_name': 'AveragePooling1D',\n",
       "  'config': {'name': 'average_pooling1d',\n",
       "   'trainable': True,\n",
       "   'batch_input_shape': [None, 6, 1],\n",
       "   'dtype': 'float32',\n",
       "   'strides': [1],\n",
       "   'pool_size': [2],\n",
       "   'padding': 'same',\n",
       "   'data_format': 'channels_last'}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "model_json = json.loads(model.to_json())\n",
    "model_json['config']['layers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[1.0,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_array.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.reshape(input_array,[1,6,1])\n",
    "\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(predictions)\n",
    "print(\"process time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Pooling 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we repeat the building process but instead of a 1D average pooling layer we will build a neural network only consisting of one single average pooling 2D layer. The created model can then be again used to validate the implementation of the average pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will prepare all the necessary data to build and train this simple neural network. Since the pooling layer is a linear process, no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1]])\n",
    "data_y = np.array([[[1],[1]],[[1],[1]],[[1],[1]],[[1],[1]]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,5,3,1])\n",
    "data_y = tf.reshape(data_y,[-1,4,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the average pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.AveragePooling2D(pool_size=(2,2), strides=(1,1),padding='valid', input_shape = (5,3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/avg_pool_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[26,1,37], [115,189,31.3], [31.3,29.6,103], [0.205,0,83], [41,36,2.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_array.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.reshape(input_array,[1,5,3,1])\n",
    "\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(predictions)\n",
    "print(\"process time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will build a neural network only consisting of one single max pooling 1D layer. The created model can then be used to validate the implementation of the max pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step we will prepare all the necessary data to build and train this simple neural network. Since the pooling layer is a linear process, no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,2,3,4,5,6]])\n",
    "data_y = np.array([[1,2,3]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,6,1])\n",
    "data_y = tf.reshape(data_y,[-1,3,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.MaxPooling1D(pool_size=2, strides=2 ,padding='valid', input_shape = (6,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/max_pool_1d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[1.0,2,3,4,5,6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our arduino ide. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_array.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.reshape(input_array,[1,6,1])\n",
    "\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(predictions)\n",
    "print(\"process time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we repeat the building process but instead of a 1D max pooling layer we will build a neural network only consisting of one single max pooling 2D layer. The created model can then be again used to validate the implementation of the max pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will prepare all the necessary data to build and train this simple neural network. Since the pooling layer is a linear process, no learning will take place in this step. Hence, it does not matter which data we will use. The only thing we have to make sure is that the shape of the input and output data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = np.array([[1,1,1], [1,1,1], [1,1,1], [1,1,1], [1,1,1]])\n",
    "data_y = np.array([[[1],[1]],[[1],[1]],[[1],[1]],[[1],[1]]])\n",
    "\n",
    "data_x = scale(data_x,axis=0)\n",
    "data_x = tf.reshape(data_x,[-1,5,3,1])\n",
    "data_y = tf.reshape(data_y,[-1,4,2,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will actually build the model. You can change the different parameters of the layer to play around with different settings and fully test the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2,2), strides=(1,1),padding='valid', input_shape = (5,3,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet will throw a warning since we do not actually learn anything. Since this is the planned behaviour, the warning can be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_x, data_y, epochs=1, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now save our model to a .h5 file to translate it with the neural network translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/max_pool_2d.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will prepare a sample input to check if the result of our build model is equal to the result of our translated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = np.array([[26,1,37], [115,189,31.3], [31.3,29.6,103], [0.205,0,83], [41,36,2.2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it more convenient, we will prepare the input data so that we can simply copy and paste the values in the serial dialog of our Arduino IDE. We can now simply copy and paste the values inbetween of the square brackets [ ]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_array.flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will perform the prediction with the trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.reshape(input_array,[1,5,3,1])\n",
    "\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(predictions)\n",
    "print(\"process time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare the output of our trained model with the output of the neural network translator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Layer Diabates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we will create a sample neural network which consists of two dense layers. Thereby, the provided diabetes.csv file ist used as training and test data. In the first step, we will load the dataset and split it into the feature vector and the result vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"diabetes.csv\", delimiter=\",\", skiprows=1 )\n",
    "diabetes_X = dataset[:,0:8]\n",
    "diabetes_Y = dataset[:,8]\n",
    "diabetes_X = scale(diabetes_X,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create a sequential neural network consisting of two dense layers with sigmoid activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(8, input_dim=8, activation='sigmoid'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(diabetes_X, diabetes_Y, epochs=300, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/2_layer_diabetes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we extract the first line of the dataset and create a sample feature vector to perform a prediction. In addition to that, we will print the input values in order to be able to copy them to the model flashed on the arduino. Finally, the prediction results, as well as, the processing time will be printed to combare the framework with the translated model created by the neural network translator for the Arduino microcontroller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = dataset[0][0:8]\n",
    "input = tf.reshape(input_array,[1,8])\n",
    "print(\"Input: \" + str(np.array(input[0]).flatten().tolist()))\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(\"Preditction: \" + str(predictions))\n",
    "print(\"Processing time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Layer Diabates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the 2-layer-diabetes neural network, a 3-layer-diabetes neural network can be created to test the neural network translator. Not only does this neural network have an additional dense layer, but it also uses different activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"diabetes.csv\", delimiter=\",\", skiprows=1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and save the neural network model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(16, input_dim=8, activation='relu'))\n",
    "model.add(keras.layers.Dense(8, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(diabetes_X, diabetes_Y, epochs=300, batch_size=10)\n",
    "model.save('out/3_layer_diabetes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = dataset[0][0:8]\n",
    "input = tf.reshape(input_array,[1,8])\n",
    "print(\"Input: \" + str(np.array(input[0]).flatten().tolist()))\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(\"Preditction: \" + str(predictions))\n",
    "print(\"Processing time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last model we will create is a sequential neural network. Therefore, we will use the known fashion mnist dataset. In the first step, we will load the dataset, which is included in the keras framework, and prepare our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0\n",
    "\n",
    "train_images = tf.reshape(train_images, [-1, 28, 28, 1])\n",
    "test_images = tf.reshape(test_images, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.imshow(train_images[i].numpy(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create the sequential neural network with three layers - an average pooling 2d layer, a flatten layer and a dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.AveragePooling2D(input_shape=(28, 28, 1), pool_size=(4,4),strides=(4,4), padding='valid', data_format='channels_last'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('out/fashion_mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After saving the model, we will now have a closer look at the input features. Therefore, we examin the first training image. Since we are working with images here, our input would be 28x28 pixels (2 dimensional). In the following, an exemplary input image is shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_image = train_images[0]\n",
    "first_image = np.array(first_image, dtype='float')\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the current implementation of the Arduino serial controller only accepts one dimensional input arrays. That's why whe apply the flatten function to the image. This results in the following input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representation as image:\")\n",
    "first_image = train_images[0]\n",
    "first_image = np.array(first_image, dtype='float').flatten()\n",
    "pixels = first_image.reshape((1, 28*28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representation as data to use as input for the translated neural network:\")\n",
    "print(np.array(train_images[0]).flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = test_images[0][0:784]\n",
    "\n",
    "input = tf.reshape(input_array,[1,28,28,1])\n",
    "time_before = int(round(time.time_ns() / 1000))\n",
    "predictions = model.predict(input)\n",
    "time_after = int(round(time.time_ns() / 1000))\n",
    "print(\"Prediction: \" + str(predictions))\n",
    "print(\"Process time in microseconds: \" + str(time_after - time_before))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, tests have shown that the used Arduino Nano cannot handle such a large amount of input data and, therefore, a comparison of the results cannot be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
